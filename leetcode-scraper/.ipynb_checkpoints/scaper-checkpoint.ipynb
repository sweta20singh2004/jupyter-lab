{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6de10f-5e53-4001-ad6d-f76d52805edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (4.12.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m498.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m217.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.2 pandas-2.2.3 tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753a1694-b663-4cb8-91e6-d974cdc30158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Failed to fetch page 1\n",
      "Scraping page 2...\n",
      "Failed to fetch page 2\n",
      "Scraping page 3...\n",
      "Failed to fetch page 3\n",
      "Scraping page 4...\n",
      "Failed to fetch page 4\n",
      "Scraping page 5...\n",
      "Failed to fetch page 5\n",
      "Empty DataFrame\n",
      "Columns: [Question, Frequency]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Constants\n",
    "LEETCODE_DISCUSS_URL = \"https://leetcode.com/discuss/general-discussion/\"\n",
    "SEARCH_QUERY = \"google interview\"\n",
    "NUM_PAGES_TO_SCRAPE = 5  # Adjust based on how many pages you want to scrape\n",
    "\n",
    "# Regex to match LeetCode question titles\n",
    "QUESTION_REGEX = re.compile(r'https://leetcode\\.com/problems/([^/]+)/')\n",
    "\n",
    "# Store question frequencies\n",
    "question_freq = defaultdict(int)\n",
    "\n",
    "def scrape_discussion_page(page_num):\n",
    "    url = f\"{LEETCODE_DISCUSS_URL}?currentPage={page_num}&orderBy=hot&query={SEARCH_QUERY}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page {page_num}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    posts = soup.find_all('div', class_='discuss-container')\n",
    "    \n",
    "    questions = []\n",
    "    for post in posts:\n",
    "        content = post.get_text().lower()\n",
    "        if \"google\" in content and \"interview\" in content:\n",
    "            # Extract question titles from links\n",
    "            links = post.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                match = QUESTION_REGEX.search(link['href'])\n",
    "                if match:\n",
    "                    question_title = match.group(1).replace('-', ' ')\n",
    "                    questions.append(question_title)\n",
    "    \n",
    "    return questions\n",
    "\n",
    "def scrape_leetcode_discussions():\n",
    "    for page_num in range(1, NUM_PAGES_TO_SCRAPE + 1):\n",
    "        print(f\"Scraping page {page_num}...\")\n",
    "        questions = scrape_discussion_page(page_num)\n",
    "        for question in questions:\n",
    "            question_freq[question] += 1\n",
    "        time.sleep(1)  # Avoid rate-limiting\n",
    "\n",
    "def get_top_questions(n=20):\n",
    "    sorted_questions = sorted(question_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_questions[:n]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_leetcode_discussions()\n",
    "    top_questions = get_top_questions()\n",
    "    \n",
    "    # Display results\n",
    "    df = pd.DataFrame(top_questions, columns=[\"Question\", \"Frequency\"])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b0a94-5abe-4bb2-9c1c-593e837cdec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
