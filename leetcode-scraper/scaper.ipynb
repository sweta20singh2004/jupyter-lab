{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ac5b24-0ee4-4e57-ad58-a08c92b624b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/conda/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/conda/lib/python3.11/site-packages (from selenium) (2023.7.22)\n",
      "Collecting typing_extensions~=4.9 (from selenium)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.28.1-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m400.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m310.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m314.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sortedcontainers, websocket-client, typing_extensions, h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 1.6.4\n",
      "    Uninstalling websocket-client-1.6.4:\n",
      "      Successfully uninstalled websocket-client-1.6.4\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-25.1.0 h11-0.14.0 outcome-1.3.0.post0 selenium-4.28.1 sortedcontainers-2.4.0 trio-0.28.0 trio-websocket-0.11.1 typing_extensions-4.12.2 websocket-client-1.8.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6de10f-5e53-4001-ad6d-f76d52805edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (4.12.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m498.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m217.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.2 pandas-2.2.3 tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753a1694-b663-4cb8-91e6-d974cdc30158",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/selenium/webdriver/common/driver_finder.py:64\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe path is not a valid file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m path\n",
      "\u001b[0;31mValueError\u001b[0m: The path is not a valid file: /path/to/chromedriver",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNoSuchDriverException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sorted_questions[:n]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mscrape_leetcode_discussions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     top_questions \u001b[38;5;241m=\u001b[39m get_top_questions()\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Display results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m, in \u001b[0;36mscrape_leetcode_discussions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_leetcode_discussions\u001b[39m():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Set up Selenium WebDriver\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     service \u001b[38;5;241m=\u001b[39m Service(executable_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/path/to/chromedriver\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Replace with your ChromeDriver path\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_PAGES_TO_SCRAPE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[1;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/selenium/webdriver/chromium/webdriver.py:50\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[1;32m     49\u001b[0m finder \u001b[38;5;241m=\u001b[39m DriverFinder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_browser_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     51\u001b[0m     options\u001b[38;5;241m.\u001b[39mbinary_location \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mget_browser_path()\n\u001b[1;32m     52\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/selenium/webdriver/common/driver_finder.py:47\u001b[0m, in \u001b[0;36mDriverFinder.get_browser_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_binary_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/selenium/webdriver/common/driver_finder.py:78\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     77\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrowser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths\n",
      "\u001b[0;31mNoSuchDriverException\u001b[0m: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "LEETCODE_DISCUSS_URL = \"https://leetcode.com/discuss/general-discussion/\"\n",
    "SEARCH_QUERY = \"google interview\"\n",
    "NUM_PAGES_TO_SCRAPE = 5  # Adjust based on how many pages you want to scrape\n",
    "\n",
    "# Regex to match LeetCode question titles\n",
    "QUESTION_REGEX = re.compile(r'https://leetcode\\.com/problems/([^/]+)/')\n",
    "\n",
    "# Store question frequencies\n",
    "question_freq = defaultdict(int)\n",
    "\n",
    "def scrape_discussion_page(driver, page_num):\n",
    "    url = f\"{LEETCODE_DISCUSS_URL}?currentPage={page_num}&orderBy=hot&query={SEARCH_QUERY}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "    # Scroll to load all posts (adjust the range for more scrolling)\n",
    "    for _ in range(3):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    posts = driver.find_elements(By.CLASS_NAME, 'discuss-post')\n",
    "    questions = []\n",
    "    for post in posts:\n",
    "        content = post.text.lower()\n",
    "        if \"google\" in content and \"interview\" in content:\n",
    "            # Extract question titles from links\n",
    "            links = post.find_elements(By.TAG_NAME, 'a')\n",
    "            for link in links:\n",
    "                href = link.get_attribute('href')\n",
    "                if href:\n",
    "                    match = QUESTION_REGEX.search(href)\n",
    "                    if match:\n",
    "                        question_title = match.group(1).replace('-', ' ')\n",
    "                        questions.append(question_title)\n",
    "    \n",
    "    return questions\n",
    "\n",
    "def scrape_leetcode_discussions():\n",
    "    # Set up Selenium WebDriver\n",
    "    service = Service(executable_path='/path/to/chromedriver')  # Replace with your ChromeDriver path\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    for page_num in range(1, NUM_PAGES_TO_SCRAPE + 1):\n",
    "        print(f\"Scraping page {page_num}...\")\n",
    "        questions = scrape_discussion_page(driver, page_num)\n",
    "        for question in questions:\n",
    "            question_freq[question] += 1\n",
    "        time.sleep(2)  # Add delay to avoid rate-limiting\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "def get_top_questions(n=20):\n",
    "    sorted_questions = sorted(question_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_questions[:n]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_leetcode_discussions()\n",
    "    top_questions = get_top_questions()\n",
    "    \n",
    "    # Display results\n",
    "    df = pd.DataFrame(top_questions, columns=[\"Question\", \"Frequency\"])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72dbf510-c05a-400b-9e66-58268138b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cloudscraper\n",
      "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pyparsing>=2.4.7 (from cloudscraper)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: requests>=2.9.2 in /opt/conda/lib/python3.11/site-packages (from cloudscraper) (2.31.0)\n",
      "Collecting requests-toolbelt>=0.9.1 (from cloudscraper)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.9.2->cloudscraper) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.9.2->cloudscraper) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.9.2->cloudscraper) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.9.2->cloudscraper) (2023.7.22)\n",
      "Downloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m11.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m437.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m287.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, requests-toolbelt, cloudscraper\n",
      "Successfully installed cloudscraper-1.2.71 pyparsing-3.2.1 requests-toolbelt-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158b0a94-5abe-4bb2-9c1c-593e837cdec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch page 1 with status code 403\n",
      "Failed to fetch page 2 with status code 403\n",
      "Failed to fetch page 3 with status code 403\n",
      "Results saved to google_questions.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "BASE_URL = \"https://leetcode.com\"\n",
    "DISCUSSION_URL = \"https://leetcode.com/discuss/interview-question?currentPage={}&orderBy=hot\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://leetcode.com/\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\"\n",
    "}\n",
    "\n",
    "# session = requests.Session()\n",
    "session = cloudscraper.create_scraper()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "def get_google_questions(pages=5):\n",
    "    question_counter = Counter()\n",
    "    question_links = {}\n",
    "    \n",
    "    for page in range(1, pages + 1):\n",
    "        url = DISCUSSION_URL.format(page)\n",
    "        response = session.get(url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch page {page} with status code {response.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        discussions = soup.select(\"div.topic\")\n",
    "        \n",
    "        for discussion in discussions:\n",
    "            title_tag = discussion.select_one(\"a\")\n",
    "            if title_tag:\n",
    "                title = title_tag.text.strip()\n",
    "                link = BASE_URL + title_tag[\"href\"]\n",
    "                \n",
    "                if \"Google\" in title:\n",
    "                    question_counter[title] += 1\n",
    "                    question_links[title] = link\n",
    "        \n",
    "        time.sleep(random.uniform(1, 3))  # Adding delay to avoid detection\n",
    "    \n",
    "    return question_counter, question_links\n",
    "\n",
    "\n",
    "def save_results(question_counter, question_links, filename=\"google_questions.json\"):\n",
    "    data = [{\"title\": title, \"count\": count, \"link\": question_links[title]} for title, count in question_counter.most_common()]\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    \n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question_counter, question_links = get_google_questions(pages=3)\n",
    "    save_results(question_counter, question_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195cdc03-1924-4d3f-96c1-0ede667848a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
